{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some\n",
    "algorithms that are not affected by missing values.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing values in a dataset refer to the absence of information for certain observations or variables. They can occur for various reasons, such as data entry errors, sensor malfunctions, or intentional omissions.\n",
    "\n",
    "\n",
    "Handling missing values is crucial for several reasons:\n",
    "1. Biased Analysis: Ignoring missing values can lead to biased and inaccurate analyses, as it may skew the results and misrepresent the true characteristics of the data.\n",
    "2. Reduced Power: Missing values can reduce the statistical power of a study, making it harder to detect meaningful patterns or relationships.\n",
    "3. Algorithm Performance: Many machine learning algorithms cannot handle missing data and may either produce errors or provide biased results. It's essential to preprocess the data before feeding it into such algorithms.\n",
    "\n",
    "Some algorithms are inherently robust to missing data, and they can handle it without requiring imputation or other preprocessing techniques. Examples include:\n",
    "1. Decision Trees: Decision trees can naturally handle missing values during the splitting process without the need for imputation.\n",
    "2. Random Forests: Random Forests, being an ensemble of decision trees, can also handle missing values effectively.\n",
    "3. K-Nearest Neighbors (KNN): KNN imputes missing values by considering the values of their nearest neighbors.\n",
    "4. Naive Bayes: Naive Bayes is relatively robust to missing data and can still perform well in the presence of missing values.\n",
    "5. Neural Networks: Some neural network architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2: List down techniques used to handle missing data. Give an example of each with python code.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Deletion of Missing Data: This involves removing observations or variables with missing values.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame with missing values\n",
    "data = {'A': [1, 2, None, 4], 'B': [5, None, 7, 8]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Drop rows with any missing values\n",
    "df_dropna = df.dropna()\n",
    "print(\"DataFrame after dropping rows with missing values:\")\n",
    "print(df_dropna)\n",
    "```\n",
    "\n",
    "2. Imputation using Mean/Median/Mode: Filling missing values with the mean, median, or mode of the respective column.\n",
    "\n",
    "```python\n",
    "# Impute missing values with the mean\n",
    "df_fill_mean = df.fillna(df.mean())\n",
    "print(\"\\nDataFrame after imputing missing values with mean:\")\n",
    "print(df_fill_mean)\n",
    "```\n",
    "\n",
    "3. Forward Fill or Backward Fill: Propagate the last known value forward or use the next known value backward to fill missing values.\n",
    "\n",
    "```python\n",
    "# Forward fill missing values\n",
    "df_ffill = df.ffill()\n",
    "print(\"\\nDataFrame after forward filling missing values:\")\n",
    "print(df_ffill)\n",
    "```\n",
    "\n",
    "4. Interpolation: Estimate missing values based on the values of other data points using interpolation techniques.\n",
    "\n",
    "```python\n",
    "# Interpolate missing values using linear interpolation\n",
    "df_interpolate = df.interpolate()\n",
    "print(\"\\nDataFrame after linear interpolation of missing values:\")\n",
    "print(df_interpolate)\n",
    "```\n",
    "\n",
    "5. K-Nearest Neighbors (KNN) Imputation: Impute missing values by considering the values of their nearest neighbors.\n",
    "\n",
    "```python\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Sample DataFrame with missing values\n",
    "data = {'A': [1, 2, None, 4], 'B': [5, None, 7, 8]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# KNN imputation\n",
    "knn_imputer = KNNImputer(n_neighbors=2)\n",
    "df_knn_imputed = pd.DataFrame(knn_imputer.fit_transform(df), columns=df.columns)\n",
    "print(\"\\nDataFrame after KNN imputation of missing values:\")\n",
    "print(df_knn_imputed)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imbalanced data refers to a situation in a classification problem where the distribution of class labels is not approximately equal. In other words, one class has significantly more instances than the other class or classes. For example, in a binary classification problem, if 95% of the samples belong to Class A and only 5% belong to Class B, the data is considered imbalanced.\n",
    "\n",
    "Consequences of not handling imbalanced data:\n",
    "\n",
    "1. Biased Model Performance: The model tends to be biased towards the majority class since it has more examples to learn from. As a result, the model may struggle to correctly classify instances from the minority class.\n",
    "\n",
    "2. Poor Generalization: Models trained on imbalanced data may not generalize well to new, unseen data, especially for the minority class. The model may make overly optimistic predictions, assuming that the majority class is always the correct prediction.\n",
    "\n",
    "3. Misleading Evaluation Metrics: Accuracy, a commonly used metric, may be misleading in imbalanced datasets. A model that predicts the majority class all the time can still achieve high accuracy, even though it's not providing meaningful insights.\n",
    "\n",
    "4. Model Skewing: Imbalanced data can cause models to be skewed towards the majority class, leading to a lack of sensitivity to the minority class. This is particularly problematic in scenarios where the minority class is of greater interest (e.g., fraud detection, rare diseases).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down-\n",
    "sampling are required.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Up-sampling** and **down-sampling** are two common techniques used to address imbalanced datasets:\n",
    "\n",
    "1. **Up-sampling:**\n",
    "   - Up-sampling involves increasing the number of instances in the minority class to match the number of instances in the majority class. This is done by generating synthetic samples or replicating existing samples from the minority class.\n",
    "\n",
    "\n",
    "2. **Down-sampling:**\n",
    "   - Down-sampling involves reducing the number of instances in the majority class, typically by randomly removing instances. The goal is to match the number of instances in the majority class to that of the minority class.\n",
    "\n",
    "\n",
    "**When to use Up-sampling and Down-sampling:**\n",
    "\n",
    "- **Up-sampling:**\n",
    "  - Use up-sampling when the minority class is underrepresented and you want to give the model more examples to learn from.\n",
    "  - It is particularly useful when the available data is limited, and you don't want to lose information by removing instances.\n",
    "\n",
    "- **Down-sampling:**\n",
    "  - Use down-sampling when the majority class has a large number of instances, and you want to prevent it from dominating the learning process.\n",
    "  - It is suitable when you have a sufficiently large dataset, and removing some instances from the majority class will not significantly impact the overall information content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5: What is data Augmentation? Explain SMOTE.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data augmentation** is a technique used to artificially increase the size of a dataset by applying various transformations to the existing data. This is commonly used in image data but can be adapted to other types of data as well. The goal of data augmentation is to introduce diversity into the dataset, allowing the model to generalize better to unseen examples.\n",
    "\n",
    "**SMOTE (Synthetic Minority Over-sampling Technique):**\n",
    "\n",
    "SMOTE is a specific data augmentation technique designed to address imbalanced datasets, particularly in the context of machine learning classification tasks. It focuses on the minority class by generating synthetic instances to balance the class distribution. SMOTE works by creating synthetic samples that are combinations of existing minority class instances.\n",
    "\n",
    "\n",
    "1. **Select a Minority Instance:**\n",
    "   - Randomly choose an instance from the minority class.\n",
    "\n",
    "2. **Find Neighbors:**\n",
    "   - Identify the k-nearest neighbors (usually k=5) of the selected instance. These are the instances that are most similar to the chosen one.\n",
    "\n",
    "3. **Generate Synthetic Instances:**\n",
    "   - For each neighbor, create a synthetic instance by linearly interpolating between the selected instance and its neighbor. The synthetic instance is created by multiplying the difference between the feature values by a random number between 0 and 1 and adding it to the feature values of the selected instance.\n",
    "\n",
    "4. **Repeat:**\n",
    "   - Repeat steps 1-3 until the desired balance between classes is achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6: What are outliers in a dataset? Why is it essential to handle outliers?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outliers** in a dataset are data points that significantly differ from the majority of other data points. They can be unusually high or low values in a dataset and may distort the overall pattern and interpretation of the data. Outliers can arise due to various reasons, including measurement errors, data entry mistakes, or genuine extreme values in the underlying population.\n",
    "\n",
    "**Reasons why it's essential to handle outliers:**\n",
    "\n",
    "1. **Impact on Statistical Measures:**\n",
    "   - Outliers can greatly influence statistical measures such as the mean and standard deviation. The mean is particularly sensitive to extreme values, and the presence of outliers can skew the mean, leading to a misrepresentation of the central tendency of the data.\n",
    "\n",
    "2. **Distortion of Data Distribution:**\n",
    "   - Outliers can distort the distribution of the data, making it difficult to accurately represent the underlying pattern. This can affect the performance of machine learning models that assume a certain distribution of the data.\n",
    "\n",
    "3. **Impact on Parametric Models:**\n",
    "   - Parametric models, which make assumptions about the distribution of the data, may be adversely affected by outliers. Outliers can violate the assumptions of normality and homoscedasticity, leading to unreliable parameter estimates.\n",
    "\n",
    "4. **Model Performance:**\n",
    "   - Outliers can have a significant impact on the performance of predictive models. For example, in linear regression, outliers can disproportionately influence the regression line, leading to a model that does not generalize well to new data.\n",
    "\n",
    "5. **Misleading Insights:**\n",
    "   - Outliers can lead to incorrect interpretations and insights about the data. Handling outliers is crucial for obtaining a more accurate understanding of the underlying patterns and trends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q7: You are working on a project that requires analyzing customer data. However, you notice that some of\n",
    "the data is missing. What are some techniques you can use to handle the missing data in your analysis?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling missing data is a crucial step in the analysis of customer data to ensure accurate and meaningful results. Here are several techniques you can use to address missing data:\n",
    "\n",
    "1. **Data Imputation:**\n",
    "   - Fill in missing values with estimated or calculated values. Common imputation techniques include:\n",
    "     - **Mean, Median, or Mode Imputation:** Fill missing values with the mean, median, or mode of the respective column.\n",
    "       ```python\n",
    "       df['column_name'].fillna(df['column_name'].mean(), inplace=True)\n",
    "       ```\n",
    "\n",
    "     - **Forward Fill or Backward Fill (for time series data):** Propagate the last known value forward or use the next known value backward to fill missing values.\n",
    "       ```python\n",
    "       df['column_name'].ffill(inplace=True)\n",
    "       ```\n",
    "\n",
    "     - **Interpolation:** Estimate missing values based on the values of other data points using interpolation techniques.\n",
    "       ```python\n",
    "       df['column_name'].interpolate(inplace=True)\n",
    "       ```\n",
    "\n",
    "2. **Deletion:**\n",
    "   - Remove rows or columns with missing values. This is suitable when the amount of missing data is small and random.\n",
    "     ```python\n",
    "     df.dropna(inplace=True)\n",
    "     ```\n",
    "\n",
    "3. **Data Augmentation:**\n",
    "   - Generate synthetic data points to replace or supplement missing values. This is particularly useful when dealing with time series or numerical data.\n",
    "     ```python\n",
    "     from sklearn.impute import KNNImputer\n",
    "\n",
    "     knn_imputer = KNNImputer(n_neighbors=2)\n",
    "     df_imputed = pd.DataFrame(knn_imputer.fit_transform(df), columns=df.columns)\n",
    "     ```\n",
    "\n",
    "4. **Prediction Models:**\n",
    "   - Use machine learning models to predict missing values based on other features. This can be effective when relationships between variables are complex.\n",
    "     ```python\n",
    "     from sklearn.ensemble import RandomForestRegressor\n",
    "     from sklearn.impute import SimpleImputer\n",
    "\n",
    "     imputer = SimpleImputer(strategy='mean')\n",
    "     df['column_name'] = imputer.fit_transform(df[['feature1', 'feature2']])\n",
    "     ```\n",
    "\n",
    "5. **Category Imputation:**\n",
    "   - For categorical data, you can replace missing values with a new category or the most frequent category.\n",
    "     ```python\n",
    "     df['categorical_column'].fillna('Unknown', inplace=True)\n",
    "     ```\n",
    "\n",
    "6. **Multiple Imputation:**\n",
    "   - Perform multiple imputations to account for uncertainty in the imputation process. This involves creating multiple datasets with different imputed values and combining the results.\n",
    "     ```python\n",
    "     from sklearn.experimental import enable_iterative_imputer\n",
    "     from sklearn.impute import IterativeImputer\n",
    "\n",
    "     imputer = IterativeImputer()\n",
    "     df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "     ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are\n",
    "some strategies you can use to determine if the missing data is missing at random or if there is a pattern\n",
    "to the missing data?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Visual Inspection:**\n",
    "   - Visualizing the missingness pattern using graphical tools can provide insights. Use heatmaps or missing data matrices to observe if there are patterns in missing values across variables or specific subsets of the data.\n",
    "     ```python\n",
    "     import seaborn as sns\n",
    "     import matplotlib.pyplot as plt\n",
    "\n",
    "     sns.heatmap(df.isnull(), cbar=False, cmap='viridis')\n",
    "     plt.show()\n",
    "     ```\n",
    "\n",
    "2. **Descriptive Statistics:**\n",
    "   - Analyze summary statistics for variables with missing values and compare them to variables without missing values. If there are systematic differences, it may suggest a non-random pattern.\n",
    "     ```python\n",
    "     df.describe(include='all')\n",
    "     ```\n",
    "\n",
    "3. **Correlation Analysis:**\n",
    "   - Examine the correlation between missing values in different variables. If there is a correlation, it may indicate a pattern in the missing data.\n",
    "     ```python\n",
    "     df.corr()\n",
    "     ```\n",
    "\n",
    "4. **Missingness Tests:**\n",
    "   - Conduct statistical tests to assess whether the missing data is completely random or if there is a systematic pattern. The Little's MCAR test is commonly used.\n",
    "     ```python\n",
    "     from missingpy import MissForest\n",
    "\n",
    "     miss_forest = MissForest()\n",
    "     imputed_data = miss_forest.fit_transform(df)\n",
    "     ```\n",
    "\n",
    "5. **Pattern Recognition Models:**\n",
    "   - Train machine learning models to predict missing values based on other features. If the model performs well, it suggests a pattern in the missing data.\n",
    "     ```python\n",
    "     from sklearn.ensemble import RandomForestClassifier\n",
    "     from sklearn.model_selection import train_test_split\n",
    "     from sklearn.metrics import accuracy_score\n",
    "\n",
    "     # Assuming 'target_variable' is the variable with missing values\n",
    "     df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "     # Train a model to predict missingness\n",
    "     model = RandomForestClassifier()\n",
    "     model.fit(df_train.drop('target_variable', axis=1), df_train['target_variable'])\n",
    "\n",
    "     # Predict missing values in the test set\n",
    "     predictions = model.predict(df_test.drop('target_variable', axis=1))\n",
    "\n",
    "     # Evaluate the model performance\n",
    "     accuracy = accuracy_score(df_test['target_variable'], predictions)\n",
    "     ```\n",
    "\n",
    "6. **Domain Knowledge:**\n",
    "   - Leverage domain knowledge to understand if there are reasons why data might be missing systematically. For example, certain types of customers may be less likely to provide certain information.\n",
    "\n",
    "7. **Interview or Survey:**\n",
    "   - If feasible, directly interview or survey individuals associated with the data to understand the reasons behind missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the\n",
    "dataset do not have the condition of interest, while a small percentage do. What are some strategies you\n",
    "can use to evaluate the performance of your machine learning model on this imbalanced dataset?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Use Appropriate Evaluation Metrics:**\n",
    "   - **Precision, Recall, and F1 Score:** These metrics are often more informative than accuracy when dealing with imbalanced datasets. Precision measures the accuracy of positive predictions, recall measures the ability to capture all positive instances, and F1 score is the harmonic mean of precision and recall.\n",
    "     ```python\n",
    "     from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "     precision = precision_score(y_true, y_pred)\n",
    "     recall = recall_score(y_true, y_pred)\n",
    "     f1 = f1_score(y_true, y_pred)\n",
    "     ```\n",
    "\n",
    "   - **Area Under the Precision-Recall Curve (AUC-PR):** The AUC-PR score is useful for imbalanced datasets, especially when the negative class (non-occurrence of the condition) dominates.\n",
    "     ```python\n",
    "     from sklearn.metrics import precision_recall_curve, auc\n",
    "\n",
    "     precision, recall, _ = precision_recall_curve(y_true, y_scores)\n",
    "     auc_pr = auc(recall, precision)\n",
    "     ```\n",
    "\n",
    "2. **Confusion Matrix Analysis:**\n",
    "   - Examine the confusion matrix to understand the true positive, true negative, false positive, and false negative rates. This can provide insights into where the model is making errors.\n",
    "     ```python\n",
    "     from sklearn.metrics import confusion_matrix\n",
    "\n",
    "     conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "     ```\n",
    "\n",
    "3. **Stratified Sampling and Cross-Validation:**\n",
    "   - Use stratified sampling or stratified k-fold cross-validation to ensure that each fold or sample maintains the same class distribution as the original dataset. This helps prevent overfitting to the majority class.\n",
    "     ```python\n",
    "     from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "     stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "     cv_scores = cross_val_score(model, X, y, cv=stratified_kfold, scoring='f1')\n",
    "     ```\n",
    "\n",
    "4. **Class Weights:**\n",
    "   - Adjust class weights in the machine learning model to penalize misclassifications of the minority class more than the majority class. This helps the model focus on the class of interest.\n",
    "     ```python\n",
    "     from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "     class_weights = {0: 1, 1: 10}  # Adjust weights based on the imbalance\n",
    "     model = RandomForestClassifier(class_weight=class_weights)\n",
    "     ```\n",
    "\n",
    "5. **Ensemble Methods:**\n",
    "   - Utilize ensemble methods like Random Forest or Gradient Boosting, which are robust to imbalanced datasets and can adapt to the skewed class distribution.\n",
    "     ```python\n",
    "     from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "     model = RandomForestClassifier()\n",
    "     ```\n",
    "\n",
    "6. **Resampling Techniques:**\n",
    "   - Experiment with resampling techniques like up-sampling the minority class or down-sampling the majority class to balance the dataset.\n",
    "     ```python\n",
    "     from imblearn.over_sampling import SMOTE\n",
    "\n",
    "     smote = SMOTE(sampling_strategy=0.5)\n",
    "     X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "     ```\n",
    "\n",
    "7. **Threshold Adjustment:**\n",
    "   - Adjust the classification threshold to influence the trade-off between precision and recall. This can be particularly useful depending on the specific requirements of the medical diagnosis project.\n",
    "     ```python\n",
    "     # Adjust the threshold for binary classification\n",
    "     y_pred_adjusted = (model.predict_proba(X)[:, 1] > threshold).astype(int)\n",
    "     ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is\n",
    "unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to\n",
    "balance the dataset and down-sample the majority class?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Random Under-Sampling:**\n",
    "   - Randomly remove instances from the majority class until a more balanced distribution is achieved.\n",
    "     ```python\n",
    "     from sklearn.utils import resample\n",
    "\n",
    "     df_majority = df[df['satisfaction'] == 'satisfied']\n",
    "     df_minority = df[df['satisfaction'] == 'unsatisfied']\n",
    "\n",
    "     # Down-sample the majority class\n",
    "     df_majority_downsampled = resample(df_majority, replace=False, n_samples=len(df_minority), random_state=42)\n",
    "\n",
    "     # Combine the down-sampled majority class with the minority class\n",
    "     df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
    "     ```\n",
    "\n",
    "2. **Under-Sampling with imbalanced-learn:**\n",
    "   - The `imbalanced-learn` library provides a convenient `RandomUnderSampler` class for under-sampling.\n",
    "     ```python\n",
    "     from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "     X = df.drop('satisfaction', axis=1)\n",
    "     y = df['satisfaction']\n",
    "\n",
    "     under_sampler = RandomUnderSampler(sampling_strategy=1.0)  # 1.0 means balancing the classes\n",
    "     X_downsampled, y_downsampled = under_sampler.fit_resample(X, y)\n",
    "     ```\n",
    "\n",
    "3. **Cluster Centroids:**\n",
    "   - The `ClusterCentroids` method in `imbalanced-learn` downsamples the majority class by replacing a cluster of majority samples with the cluster centroid.\n",
    "     ```python\n",
    "     from imblearn.under_sampling import ClusterCentroids\n",
    "\n",
    "     cluster_centroids = ClusterCentroids(sampling_strategy=1.0)\n",
    "     X_downsampled, y_downsampled = cluster_centroids.fit_resample(X, y)\n",
    "     ```\n",
    "\n",
    "4. **Tomek Links:**\n",
    "   - Tomek Links can be used to identify and remove instances from the majority class that are close to instances in the minority class.\n",
    "     ```python\n",
    "     from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "     tomek_links = TomekLinks()\n",
    "     X_downsampled, y_downsampled = tomek_links.fit_resample(X, y)\n",
    "     ```\n",
    "\n",
    "5. **NearMiss:**\n",
    "   - The NearMiss algorithm selects samples from the majority class that are close to the decision boundary, effectively down-sampling the majority class.\n",
    "     ```python\n",
    "     from imblearn.under_sampling import NearMiss\n",
    "\n",
    "     near_miss = NearMiss(version=3)\n",
    "     X_downsampled, y_downsampled = near_miss.fit_resample(X, y)\n",
    "     ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a\n",
    "project that requires you to estimate the occurrence of a rare event. What methods can you employ to\n",
    "balance the dataset and up-sample the minority class?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Random Over-Sampling:**\n",
    "   - Randomly duplicate instances from the minority class until a more balanced distribution is achieved.\n",
    "     ```python\n",
    "     from sklearn.utils import resample\n",
    "\n",
    "     df_majority = df[df['occurrence'] == 0]\n",
    "     df_minority = df[df['occurrence'] == 1]\n",
    "\n",
    "     # Up-sample the minority class\n",
    "     df_minority_upsampled = resample(df_minority, replace=True, n_samples=len(df_majority), random_state=42)\n",
    "\n",
    "     # Combine the up-sampled minority class with the majority class\n",
    "     df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
    "     ```\n",
    "\n",
    "2. **Over-Sampling with imbalanced-learn:**\n",
    "   - The `imbalanced-learn` library provides a convenient `RandomOverSampler` class for over-sampling.\n",
    "     ```python\n",
    "     from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "     X = df.drop('occurrence', axis=1)\n",
    "     y = df['occurrence']\n",
    "\n",
    "     over_sampler = RandomOverSampler(sampling_strategy=1.0)  # 1.0 means balancing the classes\n",
    "     X_upsampled, y_upsampled = over_sampler.fit_resample(X, y)\n",
    "     ```\n",
    "\n",
    "3. **SMOTE (Synthetic Minority Over-sampling Technique):**\n",
    "   - SMOTE generates synthetic instances of the minority class by interpolating between existing instances.\n",
    "     ```python\n",
    "     from imblearn.over_sampling import SMOTE\n",
    "\n",
    "     smote = SMOTE(sampling_strategy=1.0)  # 1.0 means balancing the classes\n",
    "     X_upsampled, y_upsampled = smote.fit_resample(X, y)\n",
    "     ```\n",
    "\n",
    "4. **ADASYN (Adaptive Synthetic Sampling):**\n",
    "   - ADASYN is an extension of SMOTE that adapts the amount of synthetic data based on the density of the minority class.\n",
    "     ```python\n",
    "     from imblearn.over_sampling import ADASYN\n",
    "\n",
    "     adasyn = ADASYN(sampling_strategy=1.0)\n",
    "     X_upsampled, y_upsampled = adasyn.fit_resample(X, y)\n",
    "     ```\n",
    "\n",
    "5. **SMOTENC:**\n",
    "   - SMOTENC is an extension of SMOTE that supports categorical features.\n",
    "     ```python\n",
    "     from imblearn.over_sampling import SMOTENC\n",
    "\n",
    "     smotenc = SMOTENC(sampling_strategy=1.0, categorical_features=[0, 1, 2])\n",
    "     X_upsampled, y_upsampled = smotenc.fit_resample(X, y)\n",
    "     ```\n",
    "\n",
    "6. **SMOTE + Tomek Links:**\n",
    "   - Combine SMOTE with Tomek Links to both up-sample the minority class and remove instances that form Tomek Links with the majority class, improving the decision boundary.\n",
    "     ```python\n",
    "     from imblearn.combine import SMOTETomek\n",
    "\n",
    "     smote_tomek = SMOTETomek(sampling_strategy=1.0)\n",
    "     X_upsampled, y_upsampled = smote_tomek.fit_resample(X, y)\n",
    "     ```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
